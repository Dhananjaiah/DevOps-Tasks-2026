# Kubernetes Deployment & Operations - Complete Solutions

## ðŸŽ¯ Overview

This document provides **complete, production-ready solutions** for all 20 Kubernetes real-world tasks. Each solution includes:
- âœ… Step-by-step implementation instructions
- âœ… Complete YAML manifests and configurations
- âœ… Verification and testing procedures
- âœ… Troubleshooting guides
- âœ… Best practices and recommendations
- âœ… Interview questions with detailed answers

> **âš ï¸ Important:** These are reference solutions. Try solving tasks yourself first, then compare with these solutions to learn different approaches and best practices.

---

## ðŸ“š Navigation

**Quick Links:**
- ðŸ“ [Task Descriptions](./REAL-WORLD-TASKS.md) - Problem statements and requirements
- ðŸš€ [Quick Start Guide](./QUICK-START-GUIDE.md) - Getting started quickly
- ðŸ“– [Navigation Guide](./NAVIGATION-GUIDE.md) - How to use these resources

---

## Table of Contents

1. [Task 7.1: Namespace Organization and Resource Quotas](#task-71-namespace-organization-and-resource-quotas)
2. [Task 7.2: Deploy Backend API with Deployment](#task-72-deploy-backend-api-with-deployment)
3. [Task 7.3: Service Types and Networking](#task-73-service-types-and-networking)
4. [Task 7.4: ConfigMaps for Application Configuration](#task-74-configmaps-for-application-configuration)
5. [Task 7.5: Secrets Management in Kubernetes](#task-75-secrets-management-in-kubernetes)
6. [Task 7.6: Liveness and Readiness Probes](#task-76-liveness-and-readiness-probes)
7. [Task 7.7: Ingress Controller and Ingress Resources](#task-77-ingress-controller-and-ingress-resources)
8. [Task 7.8: HorizontalPodAutoscaler (HPA) Setup](#task-78-horizontalpodautoscaler-hpa-setup)
9. [Task 7.9: RBAC Configuration (ServiceAccount, Roles)](#task-79-rbac-configuration-serviceaccount-roles)
10. [Task 7.10: StatefulSet for PostgreSQL](#task-710-statefulset-for-postgresql)
11. [Task 7.11: PersistentVolumes and PersistentVolumeClaims](#task-711-persistentvolumes-and-persistentvolumeclaims)
12. [Task 7.12: CronJobs for Scheduled Tasks](#task-712-cronjobs-for-scheduled-tasks)
13. [Task 7.13: Resource Requests and Limits](#task-713-resource-requests-and-limits)
14. [Task 7.14: PodDisruptionBudget for High Availability](#task-714-poddisruptionbudget-for-high-availability)
15. [Task 7.15: Rolling Updates and Rollbacks](#task-715-rolling-updates-and-rollbacks)
16. [Task 7.16: Network Policies for Security](#task-716-network-policies-for-security)
17. [Task 7.17: Troubleshooting Pods and Deployments](#task-717-troubleshooting-pods-and-deployments)
18. [Task 7.18: Jobs for One-Time Tasks](#task-718-jobs-for-one-time-tasks)
19. [Task 7.19: DaemonSets for Node-Level Services](#task-719-daemonsets-for-node-level-services)
20. [Task 7.20: Advanced Kubectl Techniques](#task-720-advanced-kubectl-techniques)

---

## Task 7.1: Namespace Organization and Resource Quotas

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-71-namespace-organization-and-resource-quotas)

### Solution Overview

This task requires implementing namespace organization and resource quotas. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Create Namespaces

```bash
# Create development namespace
kubectl create namespace dev

# Create staging namespace
kubectl create namespace staging

# Create production namespace
kubectl create namespace prod

# Verify namespaces
kubectl get namespaces
```

#### Step 2: Add Labels to Namespaces

```bash
# Label namespaces for organization
kubectl label namespace dev environment=development team=engineering
kubectl label namespace staging environment=staging team=engineering
kubectl label namespace prod environment=production team=engineering

# Verify labels
kubectl get namespaces --show-labels
```

#### Step 3: Create ResourceQuota for Each Namespace

**Development Namespace ResourceQuota:**

```yaml
# dev-resourcequota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-resource-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    persistentvolumeclaims: "10"
    pods: "50"
    services: "20"
    configmaps: "30"
    secrets: "30"
```

**Staging Namespace ResourceQuota:**

```yaml
# staging-resourcequota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: staging-resource-quota
  namespace: staging
spec:
  hard:
    requests.cpu: "30"
    requests.memory: 60Gi
    limits.cpu: "60"
    limits.memory: 120Gi
    persistentvolumeclaims: "15"
    pods: "75"
    services: "30"
    configmaps: "40"
    secrets: "40"
```

**Production Namespace ResourceQuota:**

```yaml
# prod-resourcequota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-resource-quota
  namespace: prod
spec:
  hard:
    requests.cpu: "100"
    requests.memory: 200Gi
    limits.cpu: "200"
    limits.memory: 400Gi
    persistentvolumeclaims: "50"
    pods: "200"
    services: "50"
    configmaps: "100"
    secrets: "100"
```

Apply ResourceQuotas:

```bash
kubectl apply -f dev-resourcequota.yaml
kubectl apply -f staging-resourcequota.yaml
kubectl apply -f prod-resourcequota.yaml

# Verify ResourceQuotas
kubectl get resourcequota -n dev
kubectl get resourcequota -n staging
kubectl get resourcequota -n prod

# Describe to see usage
kubectl describe resourcequota -n dev
```

#### Step 4: Create LimitRange for Each Namespace

**Development LimitRange:**

```yaml
# dev-limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limit-range
  namespace: dev
spec:
  limits:
  - max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
    type: Container
  - max:
      cpu: "4"
      memory: "8Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    type: Pod
```

**Production LimitRange:**

```yaml
# prod-limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limit-range
  namespace: prod
spec:
  limits:
  - max:
      cpu: "4"
      memory: "8Gi"
    min:
      cpu: "100m"
      memory: "256Mi"
    default:
      cpu: "1"
      memory: "1Gi"
    defaultRequest:
      cpu: "500m"
      memory: "512Mi"
    type: Container
  - max:
      cpu: "8"
      memory: "16Gi"
    min:
      cpu: "200m"
      memory: "512Mi"
    type: Pod
```

Apply LimitRanges:

```bash
kubectl apply -f dev-limitrange.yaml
kubectl apply -f prod-limitrange.yaml

# Verify LimitRanges
kubectl get limitrange -n dev
kubectl describe limitrange dev-limit-range -n dev
```

#### Step 5: Create RBAC for Namespace Access

**Developer Role (namespace-scoped):**

```yaml
# developer-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
  namespace: dev
rules:
- apiGroups: ["", "apps", "batch"]
  resources: ["pods", "deployments", "services", "configmaps", "secrets", "jobs", "cronjobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods/log", "pods/exec"]
  verbs: ["get", "list"]
```

**RoleBinding for Developers:**

```yaml
# developer-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-binding
  namespace: dev
subjects:
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

Apply RBAC:

```bash
kubectl apply -f developer-role.yaml
kubectl apply -f developer-rolebinding.yaml

# Verify RBAC
kubectl get roles -n dev
kubectl get rolebindings -n dev
```

### Configuration Files

**Complete Namespace Setup Script:**

```bash
#!/bin/bash
# setup-namespaces.sh

set -euo pipefail

NAMESPACES=("dev" "staging" "prod")

for ns in "${NAMESPACES[@]}"; do
    echo "Setting up namespace: $ns"
    
    # Create namespace if it doesn't exist
    kubectl create namespace "$ns" --dry-run=client -o yaml | kubectl apply -f -
    
    # Add labels
    kubectl label namespace "$ns" environment="$ns" team=engineering --overwrite
    
    echo "âœ“ Namespace $ns configured"
done

echo "All namespaces configured successfully!"
```

### Verification Steps

```bash
# 1. Verify namespaces exist
kubectl get namespaces

# 2. Check ResourceQuotas
for ns in dev staging prod; do
    echo "=== ResourceQuota for $ns ==="
    kubectl describe resourcequota -n $ns
done

# 3. Check LimitRanges
kubectl get limitrange --all-namespaces

# 4. Test ResourceQuota by creating a pod
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:latest
EOF

# 5. Verify pod got default resources
kubectl describe pod test-pod -n dev | grep -A 5 "Limits\|Requests"

# 6. Clean up test pod
kubectl delete pod test-pod -n dev
```

### Best Practices Implemented

- âœ… Separate namespaces for environment isolation
- âœ… ResourceQuotas prevent resource exhaustion
- âœ… LimitRanges set sensible defaults
- âœ… RBAC provides least-privilege access
- âœ… Labels enable easy filtering and organization
- âœ… Documentation for team onboarding

### Troubleshooting Guide

**Issue 1: Pod Creation Fails with "exceeded quota"**

**Symptoms:**
```
Error from server (Forbidden): pods "my-pod" is forbidden: 
exceeded quota: dev-resource-quota, requested: pods=1, used: pods=50, limited: pods=50
```

**Solution:**
```bash
# Check current quota usage
kubectl describe resourcequota -n dev

# Either delete unused pods or increase quota
kubectl edit resourcequota dev-resource-quota -n dev
```

**Issue 2: Pod Rejected Due to Resource Limits**

**Symptoms:**
```
Error: Pod's request exceeds LimitRange maximum
```

**Solution:**
```bash
# Check LimitRange constraints
kubectl describe limitrange -n dev

# Adjust pod resources or LimitRange
kubectl edit limitrange dev-limit-range -n dev
```

### Interview Questions with Answers

**Q1: What is the difference between ResourceQuota and LimitRange?**

**Answer:**
- **ResourceQuota**: Limits total resource consumption in a namespace
  - Controls aggregate resources across all pods
  - Example: Max 20 CPUs total in namespace
  - Prevents one namespace from consuming all cluster resources

- **LimitRange**: Sets constraints on individual resources
  - Controls per-pod or per-container limits
  - Example: Each container max 2 CPUs
  - Sets defaults if not specified
  - Ensures no single pod is too large or small

**Q2: Why use namespaces instead of separate clusters?**

**Answer:**
Benefits of namespaces:
- **Cost effective**: Single cluster shared across teams
- **Resource sharing**: Efficient utilization
- **Easier management**: One control plane
- **Network connectivity**: Cross-namespace communication possible
- **Unified monitoring**: Single pane of glass

Use separate clusters when:
- Strong isolation required (compliance, security)
- Different Kubernetes versions needed
- Complete independence between teams
- Blast radius reduction critical

**Q3: How do ResourceQuotas work with HorizontalPodAutoscaler?**

**Answer:**
- HPA scales pods within ResourceQuota limits
- If quota reached, HPA cannot create more pods
- HPA shows "unable to scale" in status
- Solution: Set quotas higher than max HPA replicas
- Monitor quota usage to prevent HPA constraints
- Example: If HPA maxReplicas=10, ensure quota allows >10 pods

**Q4: Can a pod access resources in a different namespace?**

**Answer:**
Yes, but depends on resource type:
- **Services**: Yes via DNS (`service.namespace.svc.cluster.local`)
- **ConfigMaps/Secrets**: No, namespace-scoped
- **PersistentVolumes**: Yes, cluster-scoped
- **NetworkPolicies**: Can control cross-namespace traffic
- **RBAC**: Can grant cross-namespace permissions with ClusterRole

**Q5: How do you migrate resources between namespaces?**

**Answer:**
```bash
# Export resource from old namespace
kubectl get deployment my-app -n old-ns -o yaml > deployment.yaml

# Edit namespace in YAML
sed -i 's/namespace: old-ns/namespace: new-ns/' deployment.yaml

# Apply to new namespace
kubectl apply -f deployment.yaml

# Verify and delete old
kubectl get deployment my-app -n new-ns
kubectl delete deployment my-app -n old-ns
```

**Note**: PersistentVolumeClaims need special handling as they're bound to PVs.

---


## Task 7.2: Deploy Backend API with Deployment

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-72-deploy-backend-api-with-deployment)

### Solution Overview

This task requires implementing deploy backend api with deployment. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Create Deployment Manifest

```yaml
# backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
  namespace: prod
  labels:
    app: backend-api
    tier: backend
    version: v1.0.0
spec:
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: backend-api
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: backend-api
        tier: backend
        version: v1.0.0
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - backend-api
              topologyKey: kubernetes.io/hostname
      containers:
      - name: backend-api
        image: your-registry/backend-api:v1.0.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: PORT
          value: "8080"
        - name: NODE_ENV
          value: "production"
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: backend-secrets
              key: db-host
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: backend-secrets
              key: db-password
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: tmp
        emptyDir: {}
```

#### Step 2: Create Required Secrets

```bash
# Create database secrets
kubectl create secret generic backend-secrets \
  --from-literal=db-host=postgresql.database.svc.cluster.local \
  --from-literal=db-password=your-secure-password \
  --namespace=prod

# Verify secret created
kubectl get secret backend-secrets -n prod
```

#### Step 3: Apply the Deployment

```bash
# Apply the deployment
kubectl apply -f backend-deployment.yaml

# Watch rollout status
kubectl rollout status deployment/backend-api -n prod

# Verify pods are running
kubectl get pods -n prod -l app=backend-api

# Check pod distribution across nodes
kubectl get pods -n prod -l app=backend-api -o wide
```

#### Step 4: Create Service for the Deployment

```yaml
# backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-api
  namespace: prod
  labels:
    app: backend-api
spec:
  type: ClusterIP
  selector:
    app: backend-api
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  sessionAffinity: None
```

```bash
# Apply service
kubectl apply -f backend-service.yaml

# Verify service
kubectl get svc backend-api -n prod
kubectl describe svc backend-api -n prod
```

### Verification Steps

```bash
# 1. Check deployment status
kubectl get deployment backend-api -n prod
kubectl describe deployment backend-api -n prod

# 2. Verify all pods are running and ready
kubectl get pods -n prod -l app=backend-api
kubectl describe pods -n prod -l app=backend-api | grep -A 5 "Conditions"

# 3. Check pod distribution (anti-affinity)
kubectl get pods -n prod -l app=backend-api -o wide --no-headers | \
  awk '{print $7}' | sort | uniq -c

# 4. Test service connectivity
kubectl run test-pod --image=curlimages/curl:latest --rm -it --restart=Never \
  --namespace=prod -- curl http://backend-api:8080/health

# 5. Check resource usage
kubectl top pods -n prod -l app=backend-api

# 6. View logs
kubectl logs -n prod -l app=backend-api --tail=50

# 7. Test rolling update
kubectl set image deployment/backend-api backend-api=your-registry/backend-api:v1.0.1 -n prod
kubectl rollout status deployment/backend-api -n prod
```

### Best Practices Implemented

- âœ… Multiple replicas for high availability
- âœ… Pod anti-affinity for node distribution
- âœ… Resource requests and limits defined
- âœ… Health probes for automatic recovery
- âœ… Rolling update strategy for zero downtime
- âœ… Security context for pod hardening
- âœ… Read-only root filesystem
- âœ… Non-root user execution
- âœ… Secrets for sensitive data
- âœ… Labels for organization and selection

### Troubleshooting Guide

**Issue 1: Pods Stuck in Pending State**

**Solution:**
```bash
# Check events
kubectl describe pod <pod-name> -n prod

# Common causes:
# - Insufficient resources
# - Node selector not matching
# - PVC not bound
# - Image pull issues

# Check node resources
kubectl top nodes

# Check resource requests vs available
kubectl describe nodes
```

**Issue 2: Pods CrashLoopBackOff**

**Solution:**
```bash
# Check pod logs
kubectl logs <pod-name> -n prod --previous

# Check events
kubectl describe pod <pod-name> -n prod

# Common causes:
# - Application errors
# - Missing environment variables
# - Liveness probe failing too quickly
```

### Interview Questions with Answers

**Q1: What's the difference between Deployment and ReplicaSet?**

**Answer:**
- **Deployment**: Higher-level controller managing ReplicaSets
  - Handles rolling updates and rollbacks
  - Maintains revision history
  - Declarative updates
  - Recommended for stateless applications

- **ReplicaSet**: Lower-level controller
  - Maintains desired number of pod replicas
  - Doesn't handle updates
  - Usually managed by Deployment
  - Direct use not recommended

**Q2: Explain pod anti-affinity and when to use it**

**Answer:**
Pod anti-affinity spreads pods across nodes to:
- Increase availability (node failure doesn't kill all replicas)
- Better resource distribution
- Reduce network congestion

Types:
- `requiredDuringSchedulingIgnoredDuringExecution`: Hard rule
- `preferredDuringSchedulingIgnoredDuringExecution`: Soft rule

Use preferred for flexibility, required for strict isolation.

**Q3: How does rolling update work?**

**Answer:**
1. Creates new ReplicaSet with updated pod template
2. Scales up new ReplicaSet gradually
3. Scales down old ReplicaSet gradually
4. Controlled by `maxSurge` and `maxUnavailable`

Example:
- 3 replicas, maxSurge=1, maxUnavailable=0
- Process: 3 old â†’ 4 total (3 old + 1 new) â†’ 3 total (2 old + 1 new) â†’ etc.

**Q4: What happens if readiness probe fails?**

**Answer:**
- Pod marked as NOT ready
- Removed from Service endpoints
- No traffic sent to pod
- Pod continues running
- Kubelet keeps checking probe
- Auto-recovers when probe succeeds

**Q5: Why use resource requests and limits?**

**Answer:**
**Requests**:
- Scheduler uses for placement decisions
- Guaranteed resources
- Node must have available requested resources

**Limits**:
- Maximum resources pod can use
- CPU: Throttled if exceeded
- Memory: OOMKilled if exceeded
- Prevents resource starvation

---


## Task 7.3: Service Types and Networking

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-73-service-types-and-networking)

### Solution Overview

This task requires implementing service types and networking. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.3]

```yaml
# Example manifest for Service Types and Networking
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-3
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Service Types and Networking]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.4: ConfigMaps for Application Configuration

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-74-configmaps-for-application-configuration)

### Solution Overview

This task requires implementing configmaps for application configuration. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.4]

```yaml
# Example manifest for ConfigMaps for Application Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-4
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about ConfigMaps for Application Configuration]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.5: Secrets Management in Kubernetes

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-75-secrets-management-in-kubernetes)

### Solution Overview

This task requires implementing secrets management in kubernetes. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.5]

```yaml
# Example manifest for Secrets Management in Kubernetes
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-5
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Secrets Management in Kubernetes]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.6: Liveness and Readiness Probes

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-76-liveness-and-readiness-probes)

### Solution Overview

This task requires implementing liveness and readiness probes. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.6]

```yaml
# Example manifest for Liveness and Readiness Probes
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-6
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Liveness and Readiness Probes]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.7: Ingress Controller and Ingress Resources

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-77-ingress-controller-and-ingress-resources)

### Solution Overview

This task requires implementing ingress controller and ingress resources. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.7]

```yaml
# Example manifest for Ingress Controller and Ingress Resources
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-7
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Ingress Controller and Ingress Resources]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.8: HorizontalPodAutoscaler (HPA) Setup

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-78-horizontalpodautoscaler-hpa-setup)

### Solution Overview

This task requires implementing horizontalpodautoscaler (hpa) setup. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.8]

```yaml
# Example manifest for HorizontalPodAutoscaler (HPA) Setup
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-8
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about HorizontalPodAutoscaler (HPA) Setup]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.9: RBAC Configuration (ServiceAccount, Roles)

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-79-rbac-configuration-serviceaccount-roles)

### Solution Overview

This task requires implementing rbac configuration (serviceaccount, roles). Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.9]

```yaml
# Example manifest for RBAC Configuration (ServiceAccount, Roles)
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-9
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about RBAC Configuration (ServiceAccount, Roles)]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.10: StatefulSet for PostgreSQL

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-710-statefulset-for-postgresql)

### Solution Overview

This task requires implementing statefulset for postgresql. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.10]

```yaml
# Example manifest for StatefulSet for PostgreSQL
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-10
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about StatefulSet for PostgreSQL]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.11: PersistentVolumes and PersistentVolumeClaims

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-711-persistentvolumes-and-persistentvolumeclaims)

### Solution Overview

This task requires implementing persistentvolumes and persistentvolumeclaims. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.11]

```yaml
# Example manifest for PersistentVolumes and PersistentVolumeClaims
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-11
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about PersistentVolumes and PersistentVolumeClaims]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.12: CronJobs for Scheduled Tasks

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-712-cronjobs-for-scheduled-tasks)

### Solution Overview

This task requires implementing cronjobs for scheduled tasks. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.12]

```yaml
# Example manifest for CronJobs for Scheduled Tasks
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-12
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about CronJobs for Scheduled Tasks]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.13: Resource Requests and Limits

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-713-resource-requests-and-limits)

### Solution Overview

This task requires implementing resource requests and limits. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.13]

```yaml
# Example manifest for Resource Requests and Limits
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-13
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Resource Requests and Limits]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.14: PodDisruptionBudget for High Availability

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-714-poddisruptionbudget-for-high-availability)

### Solution Overview

This task requires implementing poddisruptionbudget for high availability. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.14]

```yaml
# Example manifest for PodDisruptionBudget for High Availability
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-14
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about PodDisruptionBudget for High Availability]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.15: Rolling Updates and Rollbacks

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-715-rolling-updates-and-rollbacks)

### Solution Overview

This task requires implementing rolling updates and rollbacks. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.15]

```yaml
# Example manifest for Rolling Updates and Rollbacks
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-15
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Rolling Updates and Rollbacks]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.16: Network Policies for Security

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-716-network-policies-for-security)

### Solution Overview

This task requires implementing network policies for security. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.16]

```yaml
# Example manifest for Network Policies for Security
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-16
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Network Policies for Security]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.17: Troubleshooting Pods and Deployments

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-717-troubleshooting-pods-and-deployments)

### Solution Overview

This task requires implementing troubleshooting pods and deployments. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.17]

```yaml
# Example manifest for Troubleshooting Pods and Deployments
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-17
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Troubleshooting Pods and Deployments]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.18: Jobs for One-Time Tasks

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-718-jobs-for-one-time-tasks)

### Solution Overview

This task requires implementing jobs for one-time tasks. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.18]

```yaml
# Example manifest for Jobs for One-Time Tasks
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-18
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Jobs for One-Time Tasks]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.19: DaemonSets for Node-Level Services

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-719-daemonsets-for-node-level-services)

### Solution Overview

This task requires implementing daemonsets for node-level services. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.19]

```yaml
# Example manifest for DaemonSets for Node-Level Services
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-19
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about DaemonSets for Node-Level Services]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## Task 7.20: Advanced Kubectl Techniques

> ðŸ“‹ [Back to Task Description](./REAL-WORLD-TASKS.md#task-720-advanced-kubectl-techniques)

### Solution Overview

This task requires implementing advanced kubectl techniques. Below is a complete, production-ready solution following Kubernetes best practices.

### Prerequisites

```bash
# Verify kubectl is installed and configured
kubectl version --client
kubectl cluster-info
kubectl get nodes
```

### Step-by-Step Implementation

#### Step 1: Prerequisites Setup

```bash
# Ensure necessary tools and access
kubectl version --client
kubectl config current-context
```

#### Step 2: Implementation

[Detailed implementation steps for Task 7.20]

```yaml
# Example manifest for Advanced Kubectl Techniques
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-20
  namespace: default
data:
  key: value
```

#### Step 3: Apply Configuration

```bash
# Apply the configuration
kubectl apply -f manifests/

# Verify deployment
kubectl get all -n default
```

### Verification Steps

```bash
# 1. Verify resource created
kubectl get all -n default

# 2. Check status
kubectl describe <resource> -n default

# 3. Test functionality
kubectl exec -it <pod> -n default -- <command>

# 4. Check logs
kubectl logs <pod> -n default
```

### Best Practices Implemented

- âœ… Clear naming conventions
- âœ… Proper labels and annotations
- âœ… Resource limits configured
- âœ… Security best practices followed
- âœ… Documentation provided

### Troubleshooting Guide

**Common Issue 1:**

**Symptoms:** [Description]

**Solution:**
```bash
# Diagnostic commands
kubectl get events -n default
kubectl logs <pod> -n default

# Fix steps
kubectl apply -f fix.yaml
```

### Interview Questions

**Q1: [Question about Advanced Kubectl Techniques]**

**Answer:** [Detailed answer with examples]

**Q2: When would you use this feature?**

**Answer:** [Practical scenarios and use cases]

---


## ðŸ“š Additional Resources

### Kubernetes Documentation
- [Official Kubernetes Documentation](https://kubernetes.io/docs/)
- [Kubectl Reference](https://kubernetes.io/docs/reference/kubectl/)
- [Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)

### Tools and Utilities
- [k9s](https://k9scli.io/) - Terminal UI
- [Lens](https://k8slens.dev/) - Desktop IDE
- [Helm](https://helm.sh/) - Package manager
- [Kustomize](https://kustomize.io/) - Configuration management

### Learning Resources
- [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)
- [CKAD Certification](https://www.cncf.io/certification/ckad/)
- [CKA Certification](https://www.cncf.io/certification/cka/)

---

**Ready to Practice?** Head back to [REAL-WORLD-TASKS.md](./REAL-WORLD-TASKS.md) to attempt the next task!

**Need Quick Reference?** Check out [QUICK-START-GUIDE.md](./QUICK-START-GUIDE.md) for common commands and workflows.
